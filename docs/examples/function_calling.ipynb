{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI\n",
    "import openai\n",
    "from semantic_router.utils.logger import logger\n",
    "\n",
    "# Docs # https://platform.openai.com/docs/guides/function-calling\n",
    "def llm_openai(prompt: str, model: str = \"gpt-4\") -> str:\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "            ],\n",
    "        )\n",
    "        ai_message = response.choices[0].message.content\n",
    "        if not ai_message:\n",
    "            raise Exception(\"AI message is empty\", ai_message)\n",
    "        logger.info(f\"AI message: {ai_message}\")\n",
    "        return ai_message\n",
    "    except Exception as e:\n",
    "        raise Exception(\"Failed to call OpenAI API\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Docs https://huggingface.co/docs/transformers/main_classes/text_generation\n",
    "HF_API_TOKEN = os.environ[\"HF_API_TOKEN\"]\n",
    "\n",
    "def llm_mistral(prompt: str) -> str:\n",
    "    api_url = \"https://z5t4cuhg21uxfmc3.us-east-1.aws.endpoints.huggingface.cloud/\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HF_API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        api_url,\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 1000,\n",
    "                \"temperature\": 0.2,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to call HuggingFace API\", response.text)\n",
    "\n",
    "    ai_message = response.json()[0]['generated_text']\n",
    "    if not ai_message:\n",
    "            raise Exception(\"AI message is empty\", ai_message)\n",
    "    logger.info(f\"AI message: {ai_message}\")\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to generate config from function specification using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from semantic_router.utils.logger import logger\n",
    "\n",
    "def generate_config(schema: dict) -> dict:\n",
    "    logger.info(\"Generating config...\")\n",
    "\n",
    "    class GetWeatherSchema(BaseModel):\n",
    "        location: str\n",
    "\n",
    "        class Config:\n",
    "            name = \"get_weather\"\n",
    "\n",
    "    example_schema = GetWeatherSchema.schema()\n",
    "\n",
    "    example_config = {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"utterances\": [\n",
    "            \"What is the weather like in SF?\",\n",
    "            \"What is the weather in Cyprus?\",\n",
    "            \"weather in London?\",\n",
    "            \"Tell me the weather in New York\",\n",
    "            \"what is the current weather in Paris?\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Given the following Pydantic function schema,\n",
    "    generate a config ONLY in a valid JSON format.\n",
    "    For example:\n",
    "    SCHEMA: {example_schema}\n",
    "    CONFIG: {example_config}\n",
    "\n",
    "\n",
    "    GIVEN SCHEMA: {schema}\n",
    "    GENERATED CONFIG: <generated_response_in_json>\n",
    "    \"\"\"\n",
    "\n",
    "    ai_message = llm_openai(prompt)\n",
    "    print(f\"AI message: {ai_message}\")\n",
    "\n",
    "    # Parsing for Mistral model\n",
    "    ai_message = ai_message.replace(\"CONFIG:\", \"\").replace(\"'\", '\"').strip()\n",
    "\n",
    "    try:\n",
    "        route_config = json.loads(ai_message)\n",
    "        logger.info(f\"Generated config: {route_config}\")\n",
    "        return route_config\n",
    "    except json.JSONDecodeError as json_error:\n",
    "        logger.error(f\"JSON parsing error {json_error}\")\n",
    "        print(f\"AI message: {ai_message}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract function parameters using `Mistral` open-source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(query: str, schema: dict) -> dict:\n",
    "    logger.info(\"Extracting parameters...\")\n",
    "    example_query = \"what is the weather in London?\"\n",
    "\n",
    "    class GetWeatherSchema(BaseModel):\n",
    "        location: str\n",
    "\n",
    "        class Config:\n",
    "            name = \"get_weather\"\n",
    "\n",
    "    example_schema = GetWeatherSchema.schema()\n",
    "\n",
    "    example_parameters = {\n",
    "        \"location\": \"London\",\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Given the following function schema and query, extract the parameters from the\n",
    "    query, in a valid JSON format.\n",
    "    Example:\n",
    "    SCHEMA:\n",
    "    {example_schema}\n",
    "    QUERY:\n",
    "    {example_query}\n",
    "    PARAMETERS:\n",
    "    {example_parameters}\n",
    "    GIVEN SCHEMA:\n",
    "    {schema}\n",
    "    GIVEN QUERY:\n",
    "    {query}\n",
    "    EXTRACTED PARAMETERS:\n",
    "    \"\"\"\n",
    "\n",
    "    ai_message = llm_openai(prompt)\n",
    "\n",
    "    ai_message = ai_message.replace(\"'\", '\"').strip()\n",
    "\n",
    "    try:\n",
    "        parameters = json.loads(ai_message)\n",
    "        logger.info(f\"Extracted parameters: {parameters}\")\n",
    "        return parameters\n",
    "    except json.JSONDecodeError as json_error:\n",
    "        logger.error(f\"JSON parsing error {json_error}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the routing layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_router.schema import Route\n",
    "from semantic_router.encoders import CohereEncoder\n",
    "from semantic_router.layer import RouteLayer\n",
    "from semantic_router.utils.logger import logger\n",
    "\n",
    "\n",
    "def get_route_layer(config: list[dict]) -> RouteLayer:\n",
    "    logger.info(\"Getting route layer...\")\n",
    "    encoder = CohereEncoder()\n",
    "\n",
    "    routes = []\n",
    "    print(f\"Config: {config}\")\n",
    "    for route in config:\n",
    "        if \"name\" in route and \"utterances\" in route:\n",
    "            print(f\"Route: {route}\")\n",
    "            routes.append(Route(name=route[\"name\"], utterances=route[\"utterances\"]))\n",
    "        else:\n",
    "            logger.warning(f\"Misconfigured route: {route}\")\n",
    "\n",
    "    return RouteLayer(encoder=encoder, routes=routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-14 17:28:22 INFO semantic_router.utils.logger Generating config...\u001b[0m\n",
      "\u001b[32m2023-12-14 17:28:28 INFO semantic_router.utils.logger AI message: {\"name\": \"get_time\", \"utterances\": [\"What is the time in SF?\", \"What is the current time in London?\", \"Time in Tokyo?\", \"Tell me the time in New York\", \"What is the time now in Paris?\"]}\u001b[0m\n",
      "\u001b[32m2023-12-14 17:28:28 INFO semantic_router.utils.logger Generated config: {'name': 'get_time', 'utterances': ['What is the time in SF?', 'What is the current time in London?', 'Time in Tokyo?', 'Tell me the time in New York', 'What is the time now in Paris?']}\u001b[0m\n",
      "\u001b[32m2023-12-14 17:28:28 INFO semantic_router.utils.logger Getting route layer...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI message: {\"name\": \"get_time\", \"utterances\": [\"What is the time in SF?\", \"What is the current time in London?\", \"Time in Tokyo?\", \"Tell me the time in New York\", \"What is the time now in Paris?\"]}\n",
      "Config: [{'name': 'get_time', 'utterances': ['What is the time in SF?', 'What is the current time in London?', 'Time in Tokyo?', 'Tell me the time in New York', 'What is the time now in Paris?']}]\n",
      "Route: {'name': 'get_time', 'utterances': ['What is the time in SF?', 'What is the current time in London?', 'Time in Tokyo?', 'Tell me the time in New York', 'What is the time now in Paris?']}\n",
      "None What is the weather like in Barcelona?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-14 17:28:29 INFO semantic_router.utils.logger Extracting parameters...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_time What time is it in Taiwan?\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class GetTimeSchema(BaseModel):\n",
    "    location: str\n",
    "\n",
    "    class Config:\n",
    "        name = \"get_time\"\n",
    "\n",
    "get_time_schema = GetTimeSchema.schema()\n",
    "\n",
    "def get_time(location: str) -> str:\n",
    "    # Validate parameters\n",
    "    GetTimeSchema(location=location)\n",
    "\n",
    "    print(f\"Calling get_time function with location: {location}\")\n",
    "    return \"get_time\"\n",
    "\n",
    "\n",
    "route_config = generate_config(get_time_schema)\n",
    "route_layer = get_route_layer([route_config])\n",
    "\n",
    "queries = [\n",
    "    \"What is the weather like in Barcelona?\",\n",
    "    \"What time is it in Taiwan?\",\n",
    "    \"What is happening in the world?\",\n",
    "    \"what is the time in Kaunas?\",\n",
    "    \"Im bored\",\n",
    "    \"I want to play a game\",\n",
    "    \"Banana\",\n",
    "]\n",
    "\n",
    "# Calling functions\n",
    "for query in queries:\n",
    "    function_name = route_layer(query)\n",
    "    print(function_name, query)\n",
    "\n",
    "    if function_name == \"get_time\":\n",
    "        function_parameters = extract_parameters(query, get_time_schema)\n",
    "        try:\n",
    "            # Call the function\n",
    "            get_time(**function_parameters)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
